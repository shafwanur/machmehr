{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "145cf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c51d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda'\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # max context length for predictions.\n",
    "max_iters = 6000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"{device=}\")\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a1b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# import dataset\n",
    "with open('../data/goethe.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# # unique characters that occur in the dataset\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# print(f\"{vocab_size=}\")\n",
    "\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "\n",
    "# Replace `encode` and `decode`:\n",
    "encode = lambda s: tokenizer.encode(s, add_special_tokens=False)\n",
    "decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db52e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89b1b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7962d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.392729 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d83864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9881, val loss 10.9886\n",
      "step 500: train loss 5.5143, val loss 5.6873\n",
      "step 1000: train loss 4.9188, val loss 5.4105\n",
      "step 1500: train loss 4.4884, val loss 5.2830\n",
      "step 2000: train loss 4.1537, val loss 5.2567\n",
      "step 2500: train loss 3.8691, val loss 5.2987\n",
      "step 3000: train loss 3.5995, val loss 5.3714\n",
      "step 3500: train loss 3.3525, val loss 5.4667\n",
      "step 4000: train loss 3.1292, val loss 5.5809\n",
      "step 4500: train loss 2.9177, val loss 5.6753\n",
      "step 5000: train loss 2.7262, val loss 5.8233\n",
      "step 5500: train loss 2.5535, val loss 5.9428\n",
      "step 5999: train loss 2.3914, val loss 6.0721\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create TensorBoard writer \n",
    "writer = SummaryWriter(log_dir=\"../runs/goethe_stats\")\n",
    "\n",
    "# ------- Training loop\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar(\"loss/train\", train_loss, iter)\n",
    "        writer.add_scalar(\"loss/val\", val_loss, iter)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# save the model\n",
    "model_name = \"GöPTv3.0\"\n",
    "torch.save(model.state_dict(), f\"../models/{model_name}.pt\")\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41309149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Blagen, Zu dringen Im Heilandlebung.\n",
      "Halt sogleich jener Mühmüstrung, Arkadle Beneiden, Verklärenden Rat, dicht welche des Orkus frommen Schöne Getrenntes, eigenstamme Kühlt es sicher dich alsobald niedergesehen?\n",
      "Doch was du umgibt?\n",
      "Man macht wohl, wie verworren, Vergänglich verschüchtert besser erscheint Mit frommer Schätzen holdung; Doch Obdach ist man besser viel von Ort; Doch oft sieht man, wie man bös lernen.\n",
      "Ein Gleichnis aber doch bald verstehen, als er nach Spekulation vermehrt Schwester zieht, An Vater kaum; Nie erscheint er würdig, die alten Gestalten Mit Lotten umständlich sind.\n",
      "Was sie sind auch bei Hof zu ficht hinüber?\n",
      "Hat er sich selbst In aller Weisheit wächst dies alles Schonung gedeckt; Es fällt ein großes Glück und ähnlich, Liebenderbt, Wie noch Wunder uns je gedrungen, So selten kann der Anblick bereiten, Nach Tat ein Weileins dem Boden widmen, Ein eigner, Fort Feuer; Selbst dann bedarf keiner Am alten Haupt auch als des Königreichs stols eignen ersten Eintritt.\n",
      "Dem heiligen Eichen Wozu üppig quälen seid ihr, Herrscherin, Zur Höheran mit auf uns aufwärts am wildem Saften, Wie wir umsteckt, hingekehn; Doch wessen, Rosen erblickten wir morgens Und aller Sterbliche dann der Alten, Wir acht: Empfindem Leben sagtet alles Schönen gelten!\n",
      "Denn jung!\n",
      "Glücklich laß sie uns allein Den innigen und fern; So getrost jedes in deutschen Fittlicher Hegt, Die Nächte Mund versche denn schelten dir's auch zu frühehn, Ist grün zu gut.\n",
      "Doch daß wir nicht, eh' sie schon erreichen, Laß sie geheim zu Amicken In ungenügter Wähnen.\n",
      "Entschlossenheit, wohin noch ferner flieht sie zieren soll, Ist grade alte Zeit, Die schönste Pflicht und Raum entschwemmen.\n",
      "Gewissne, Schwillt, gegen die kurz; Verlornige Welt geziehn sie herüber, Verbieten, Sie sich des Sturzes, Den Sinn, gewidmet in fremde Brust.\n",
      "Zu ihrem Tritt führt Aphigen, sämtliche Lebenshoffleitet geb' ich Am äther Stellung auf.\n",
      "Der eine Frau Jene wunderbare Stadt An meinen Zipfel, Ein sumpfes rauscht das freudige wird uns zu diesem Feste bitten, Muss vor diesem Blick, wennge Ufer Sollte noch Wunder als liebevoll Wundergestalten verheiß.\n",
      "Ihr seid in seiner Frauen fremden scheuliche Blüten, Wenn wir den rasen drinne Fürsten Durch Markt und Waldäubern.\n",
      "Wenn diese kleinen feindlichen Sonne wende, Nach Würm Lauscht, Gewandtheit, war Phantasie bedeckt.\n",
      "Denn wo Tat auf wüßten, Wo man sie dort mit gutem Anmut und Freiheit, Zerstreuungen grünt.\n",
      "Nah war dieses hingegeben Im Frost sahen der Hügel uns regelmäßig.\n",
      "Empfangt immer fort mit stürm Rücken auf In Erde, und trennte die Ahn.\n",
      "Hoch erfreute Gott rein war ein Anblick, das offenbare Denkmal der Grube tadelnd, gefühlvoller Hand.\n",
      "Und dies eben so hielt ich schon den Wunsch, als ich die jungen Frau, und grässlichen Vögel; Aber Vortrer ward von meinen Gondischen Freigeschrei in geistlichen Nachbar Stunde Gold, allen Gefahren ges- und Einsigate Ottiliens Fassung der Knecht dingende Ahn und sich auftisch- und Akademien wirbeln, wenig Witz der flachen Welt als tausend, die sehr niedergeschlagen aussinnen manche Reste zu empfinden.\n",
      "Hier und indessen werden nunmehr die Nahmen solcher Leute als Prüfungen für schon die Natur und festliches verlangt, daß man unglücklicher sind, sich im Augenblick fühlen zu helfen wissen; doch geben sie überall gleich in allen Fällen, zu empfangen.\n",
      "Dieses ist diese Klarheit ohne Kälte die Zeit so verwickelt getrieben.\n",
      "denn so daß man einen Papst hätte, aber oft lästig, um immer ausgesprochen, es zu retard und Erdboden, wenn man diesseits in sein Geist wiederbrannte, ohne irgend eine Lust zu denken scheinen.\n",
      "Man brachte das Umgänge zu sehr hoch und Reste der Natur mit ganz Wirkung hervor, zwischen Ordnung und Natur immer Abteilung die Vorstellung der Wirkung einer guten Reihe von Bau und Vorwand, Charakteren und Seiten in der man ihrer falschen Absicht zu bewirken muß, da es könne, ruhen auf bloß in der Natur zu legen und sich so freundlich entgegenkommende.\n",
      "Bald solcher Zustand bezogen sich und so weit auseinanderzuoliath und Gemäuer der Dämme, sodaß vielleicht die äußere Welt, von Sia am meisten zu dem Kleinnern bald unsterblichen Untergang gab.\n",
      "In allen Wettstreit war eine Krankheit, die Reinlichkeit des guten Kindes, die ihm Lenardo gerade bei den besten Platz zu verschaffen hatte, den Ankauf er in der Folge entweder sichtbar vorlesen.\n",
      "Endlich auch er noch seinem Abend die Legende und in der Folge entweder gefangen ist: denn in den gebildeten, bei dem Astronomen läßt das Fäßchen aus wage.\n",
      "Stit' er sich mit den goldenen Reißfedern an fremden Richter; während seines Erstaunen verwadern Tücher und seinen Grund war nicht geschmeichelt, wie ähnliche und teilweise.\n",
      " Absicht diese Bewegung; es bedächtige der katholischen Religion, welches ihm als möglich bevorsteht, war gewiß nur zu wenig Ernst.\n",
      "Die Schwierigkeit- und höchsten bis ins Gedächtnis gefertigten Kontraus zu solchem andern Frohend, wuchs von der Tat ausgezeichnet erquickt, sich mit dem Neid traulichen nächsten Erinnerung gesitudrang mit überblickenden Geschäften.\n",
      "Ich dächte, wie der ganze Zeit, des wenig Meisters dankbarlichstümliche Schwierigkeiten, nie Widersacher zu fassen und zuckte dem andern Individuum den Vorteil einer einzelnen, wo beide so glücklich sind, als sie hier zusammenzuwirken noch eine ewige Lampe stiften, doch laßlichen Schlangenweg, mehrere Personen tragen und breit ansteht, alle Hindernisse aussprechend wollen.\n",
      "So in dieser Zeit werden wir es durchhaben, Gelegenheit dem Resultate eines irdischen zieht.\n",
      "Der römische Sieger Abreise, den Toren, der auf einem so zerstreuten de Pauw, Frascati, wäre zu munter und zu erwerben; denn da es einigermaßen, als wir noch Geld genug beschäftigt, als schmeicheln könnten, weil der Verstanddrischterweise, mehr oder weniger auf uns loszukommen und Leidenschaften hervorschlendern.\n",
      "Inse zurückzukehren der jene Reise etwa eine Stunde fanden wir unser barbar beschimseßigen Freund nicht mit dem Gesetz und Edelhofe.\n",
      "Wir schelten, damit ich nicht die säumten, auf Trientasse Seele angekamen, um ein etwas rührender Morgensonierte: der Abend des aufwärtses ist nahe gekommen, munter und dient nicht einen Teil der entferntesten Bergen.\n",
      "Die Witterung war groß und munter dekoriert, es war wichtig, als er schon mehr noch einmal fertig als ziehen, um leichter zu wünschen, er möchte beinahe sagen können; den Körperlichkeit ließ seinen Jüngling alle die Stücke, ihn gehindert und sagte.\n",
      "Dann um Verzeihung: er zu groß und zu Leibesuchen, wies ihn über diese Ansicht und andere Geschichte der Fassung und über das andere.\n",
      "Schon zu zwei Jahren war die Spazierengehen; doch nur die Höhle des Vorderpferkerszenen; der Geistlichkeit meines Mundes ist versöhnt: Stellen wurden erst in tausend Jahren luftigen Hauteliere und Oberforstmeister; doch behauptet unsere Begier Wagner.\n",
      "Wie!' ich mit einiger Beruhigung: herabhängende die Lücke sind Gebote der Weiber, wenn nicht der leere rechte Ausdruck ist; ausgegrabene Deutschen, mit ihnen Innigkeit hinterhalten, Spitzbögiene; weil die Götter sich um das Volk verlangte und Erinnerungen.\n",
      "Ich lieblich, ich darf wohl sagen Offenbarungen, zu schließen von der Mutter gelernt, welches an einem Ortehalb Fuß lang gesinnt ward; desto weniger habe ich bemerkt, und wie ich's den Augen.\n",
      "Zwar empfindliche erblickt' ich ihn lange nicht, und ihm zu begegnen, bis bei Nacht war, um so nööge uns so mehr, als ob ich ihn mit ihm zugebracht hatte.\n",
      "Denn meine Relationhieß' abend unvermutet und Bilder, eben, ich fand mit der ungesprochen, wo die älteste und Menschen bewohnt ward.\n",
      "Und da nur nicht mehr Speise nur das, was sich von einem bändigen Gemüte hielt, ging mir durch Leibet wieder hervor, ich sprach mit einiger Lebhaftigkeit: Den Atems gesehn, das für Euch ohne Neid nicht mehr war.\n",
      "Sollt' ich!\n",
      "Das Leben nicht alles, mein Leben hab' ich niemals empfunden und gar leicht gewähren können.\n",
      "Zum Beispiel, wie ich mich an behor.\n",
      "Rom hats niemand das Ansehen verklagt Und vom Geist den Geist der Mut umgehangen und ihre Verhältnisse zu steigern.\n",
      "Keine Worte sei größer wollte mir immer danken, und richte fühlt' er absolut, Ver sammelnster, ich weiß es nicht, und sann es kaum das Märchen, und hätt' er alles wieder, er wollte sich näher erklären, er spricht gelassen und als künftigen Unglück.\n",
      "Das Krone ohne, wem Ehre gebeu.\n",
      "Sie kommen auf die Nachricht, aus Euch unsern Abschied zu bilden; er führt' ihn gleich, Eilesen!\n",
      "Und sie unterbrach den Vortrag freundlich und rief mit ihm: wie hastig ihn gleich kennte, Zeigtster Fürst!\n",
      "Sie traten zusammen, und sie stand am erstenmal.\n",
      "So war ihm ängstlich gewarntes Verhältnis auf und wendete ihn zu diesem allen die Wange seines Freundes, Die noch auf dem Heimwege.\n",
      "Dann spielte er den Mund auf seinen Wegen: Ich ruhre wieder!\n",
      "Er, Dreiund und Ehre ist in solchen Kleinheit, und doch geht\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "model = GPT()\n",
    "model.load_state_dict(torch.load(f\"../models/{model_name}.pt\"))\n",
    "m = model.to(device)\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cf197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
